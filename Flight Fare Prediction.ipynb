{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "521eb76b",
   "metadata": {},
   "source": [
    "# Importing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e20a76",
   "metadata": {},
   "source": [
    "#Check whether any null values are there or not. if it is present then following can be done,\n",
    "#Imputing data using Imputation method in sklearn\n",
    "#Filling NaN values with mean, median and mode using fillna() method\n",
    "#Describe data --> which can give statistical analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0f6db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f24701f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_excel(\"Data_Train.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d440cfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc15a742",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf0626d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11ed956d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"Duration\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22c2e238",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.dropna(inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aefdb548",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa1d3be",
   "metadata": {},
   "source": [
    "# EDA\n",
    "From description we can see that Date_of_Journey is a object data type,\n",
    "Therefore, we have to convert this datatype into timestamp so as to use this column properly for prediction\n",
    "\n",
    "For this we require pandas to_datetime to convert object data type to datetime dtype.\n",
    "\n",
    ".dt.day method will extract only day of that date\n",
    ".dt.month method will extract only month of that date\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58f2ae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"Journey_day\"] = pd.to_datetime(train_data.Date_of_Journey, format=\"%d/%m/%Y\").dt.day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d2fc8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"Journey_month\"] = pd.to_datetime(train_data[\"Date_of_Journey\"], format = \"%d/%m/%Y\").dt.month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a11e929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have converted Date_of_Journey column into integers, Now we can drop as it is of no use.\n",
    "\n",
    "train_data.drop([\"Date_of_Journey\"], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83845181",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af60c462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Departure time is when a plane leaves the gate. \n",
    "# Similar to Date_of_Journey we can extract values from Dep_Time\n",
    "\n",
    "# Extracting Hours\n",
    "train_data[\"Dep_hour\"] = pd.to_datetime(train_data[\"Dep_Time\"]).dt.hour\n",
    "\n",
    "# Extracting Minutes\n",
    "train_data[\"Dep_min\"] = pd.to_datetime(train_data[\"Dep_Time\"]).dt.minute\n",
    "\n",
    "# Now we can drop Dep_Time as it is of no use\n",
    "train_data.drop([\"Dep_Time\"], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d55daa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrival time is when the plane pulls up to the gate.\n",
    "# Similar to Date_of_Journey we can extract values from Arrival_Time\n",
    "\n",
    "# Extracting Hours\n",
    "train_data[\"Arrival_hour\"] = pd.to_datetime(train_data.Arrival_Time).dt.hour\n",
    "\n",
    "# Extracting Minutes\n",
    "train_data[\"Arrival_min\"] = pd.to_datetime(train_data.Arrival_Time).dt.minute\n",
    "\n",
    "# Now we can drop Arrival_Time as it is of no use\n",
    "train_data.drop([\"Arrival_Time\"], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07e898bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time taken by plane to reach destination is called Duration\n",
    "# It is the differnce betwwen Departure Time and Arrival time\n",
    "\n",
    "\n",
    "# Assigning and converting Duration column into list\n",
    "duration = list(train_data[\"Duration\"])\n",
    "\n",
    "for i in range(len(duration)):\n",
    "    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\n",
    "        if \"h\" in duration[i]:\n",
    "            duration[i] = duration[i].strip() + \" 0m\"   # Adds 0 minute\n",
    "        else:\n",
    "            duration[i] = \"0h \" + duration[i]           # Adds 0 hour\n",
    "\n",
    "duration_hours = []\n",
    "duration_mins = []\n",
    "for i in range(len(duration)):\n",
    "    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n",
    "    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))   # Extracts only minutes from duration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ee6b79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding duration_hours and duration_mins list to train_data dataframe\n",
    "\n",
    "train_data[\"Duration_hours\"] = duration_hours\n",
    "train_data[\"Duration_mins\"] = duration_mins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "305a420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop([\"Duration\"], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76918996",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c35729ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"Airline\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4340d20",
   "metadata": {},
   "source": [
    "# Handling Categorical Data\n",
    "One can find many ways to handle categorical data. Some of them categorical data are,\n",
    "\n",
    "Nominal data --> data are not in any order --> OneHotEncoder is used in this case\n",
    "Ordinal data --> data are in order --> LabelEncoder is used in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32a986bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From graph we can see that Jet Airways Business have the highest Price.\n",
    "# Apart from the first Airline almost all are having similar median\n",
    "\n",
    "# Airline vs Price\n",
    "sns.catplot(y = \"Price\", x = \"Airline\", data = train_data.sort_values(\"Price\", ascending = False), kind=\"boxen\", height = 6, aspect = 3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6eea6b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As Airline is Nominal Categorical data we will perform OneHotEncoding\n",
    "\n",
    "Airline = train_data[[\"Airline\"]]\n",
    "\n",
    "Airline = pd.get_dummies(Airline, drop_first= True)\n",
    "\n",
    "Airline.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25d1d410",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"Source\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec3bfdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source vs Price\n",
    "\n",
    "sns.catplot(y = \"Price\", x = \"Source\", data = train_data.sort_values(\"Price\", ascending = False), kind=\"boxen\", height = 4, aspect = 3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d0bf2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As Source is Nominal Categorical data we will perform OneHotEncoding\n",
    "\n",
    "Source = train_data[[\"Source\"]]\n",
    "\n",
    "Source = pd.get_dummies(Source, drop_first= True)\n",
    "\n",
    "Source.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76e7e17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"Destination\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fe352b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As Destination is Nominal Categorical data we will perform OneHotEncoding\n",
    "\n",
    "Destination = train_data[[\"Destination\"]]\n",
    "\n",
    "Destination = pd.get_dummies(Destination, drop_first = True)\n",
    "\n",
    "Destination.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80a2f07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"Route\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "759265bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional_Info contains almost 80% no_info\n",
    "# Route and Total_Stops are related to each other\n",
    "\n",
    "train_data.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)\n",
    "train_data[\"Total_Stops\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab8dd34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As this is case of Ordinal Categorical type we perform LabelEncoder\n",
    "# Here Values are assigned with corresponding keys\n",
    "\n",
    "train_data.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bec6e110",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "480fa01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate dataframe --> train_data + Airline + Source + Destination\n",
    "\n",
    "data_train = pd.concat([train_data, Airline, Source, Destination], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b367e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6144eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "080a471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81253426",
   "metadata": {},
   "source": [
    "# Test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db1810c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_excel(\"Test_set.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e5cc1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "245db8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "print(\"Test data Info\")\n",
    "print(\"-\"*75)\n",
    "print(test_data.info())\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(\"Null values :\")\n",
    "print(\"-\"*75)\n",
    "test_data.dropna(inplace = True)\n",
    "print(test_data.isnull().sum())\n",
    "\n",
    "# EDA\n",
    "\n",
    "# Date_of_Journey\n",
    "test_data[\"Journey_day\"] = pd.to_datetime(test_data.Date_of_Journey, format=\"%d/%m/%Y\").dt.day\n",
    "test_data[\"Journey_month\"] = pd.to_datetime(test_data[\"Date_of_Journey\"], format = \"%d/%m/%Y\").dt.month\n",
    "test_data.drop([\"Date_of_Journey\"], axis = 1, inplace = True)\n",
    "\n",
    "# Dep_Time\n",
    "test_data[\"Dep_hour\"] = pd.to_datetime(test_data[\"Dep_Time\"]).dt.hour\n",
    "test_data[\"Dep_min\"] = pd.to_datetime(test_data[\"Dep_Time\"]).dt.minute\n",
    "test_data.drop([\"Dep_Time\"], axis = 1, inplace = True)\n",
    "\n",
    "# Arrival_Time\n",
    "test_data[\"Arrival_hour\"] = pd.to_datetime(test_data.Arrival_Time).dt.hour\n",
    "test_data[\"Arrival_min\"] = pd.to_datetime(test_data.Arrival_Time).dt.minute\n",
    "test_data.drop([\"Arrival_Time\"], axis = 1, inplace = True)\n",
    "\n",
    "# Duration\n",
    "duration = list(test_data[\"Duration\"])\n",
    "\n",
    "for i in range(len(duration)):\n",
    "    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\n",
    "        if \"h\" in duration[i]:\n",
    "            duration[i] = duration[i].strip() + \" 0m\"   # Adds 0 minute\n",
    "        else:\n",
    "            duration[i] = \"0h \" + duration[i]           # Adds 0 hour\n",
    "\n",
    "duration_hours = []\n",
    "duration_mins = []\n",
    "for i in range(len(duration)):\n",
    "    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n",
    "    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))   # Extracts only minutes from duration\n",
    "\n",
    "# Adding Duration column to test set\n",
    "test_data[\"Duration_hours\"] = duration_hours\n",
    "test_data[\"Duration_mins\"] = duration_mins\n",
    "test_data.drop([\"Duration\"], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "# Categorical data\n",
    "\n",
    "print(\"Airline\")\n",
    "print(\"-\"*75)\n",
    "print(test_data[\"Airline\"].value_counts())\n",
    "Airline = pd.get_dummies(test_data[\"Airline\"], drop_first= True)\n",
    "print()\n",
    "\n",
    "print(\"Source\")\n",
    "print(\"-\"*75)\n",
    "print(test_data[\"Source\"].value_counts())\n",
    "Source = pd.get_dummies(test_data[\"Source\"], drop_first= True)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Destination\")\n",
    "print(\"-\"*75)\n",
    "print(test_data[\"Destination\"].value_counts())\n",
    "Destination = pd.get_dummies(test_data[\"Destination\"], drop_first = True)\n",
    "\n",
    "# Additional_Info contains almost 80% no_info\n",
    "# Route and Total_Stops are related to each other\n",
    "test_data.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)\n",
    "\n",
    "# Replacing Total_Stops\n",
    "test_data.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)\n",
    "\n",
    "# Concatenate dataframe --> test_data + Airline + Source + Destination\n",
    "data_test = pd.concat([test_data, Airline, Source, Destination], axis = 1)\n",
    "\n",
    "data_test.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(\"Shape of test data : \", data_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "94604551",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae51722",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "Finding out the best feature which will contribute and have good relation with target variable. Following are some of the feature selection methods,\n",
    "\n",
    "heatmap\n",
    "feature_importance_\n",
    "SelectKBest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "542dee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ddadcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6b8c45c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_train.loc[:, ['Total_Stops', 'Journey_day', 'Journey_month', 'Dep_hour',\n",
    "       'Dep_min', 'Arrival_hour', 'Arrival_min', 'Duration_hours',\n",
    "       'Duration_mins', 'Airline_Air India', 'Airline_GoAir', 'Airline_IndiGo',\n",
    "       'Airline_Jet Airways', 'Airline_Jet Airways Business',\n",
    "       'Airline_Multiple carriers',\n",
    "       'Airline_Multiple carriers Premium economy', 'Airline_SpiceJet',\n",
    "       'Airline_Trujet', 'Airline_Vistara', 'Airline_Vistara Premium economy',\n",
    "       'Source_Chennai', 'Source_Delhi', 'Source_Kolkata', 'Source_Mumbai',\n",
    "       'Destination_Cochin', 'Destination_Delhi', 'Destination_Hyderabad',\n",
    "       'Destination_Kolkata', 'Destination_New Delhi']]\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66a23b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_train.iloc[:, 1]\n",
    "y.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1f03667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds correlation between Independent and dependent attributes\n",
    "\n",
    "plt.figure(figsize = (18,18))\n",
    "sns.heatmap(train_data.corr(), annot = True, cmap = \"RdYlGn\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c647d1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important feature using ExtraTreesRegressor\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "selection = ExtraTreesRegressor()\n",
    "selection.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "32234da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(selection.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c11e8b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot graph of feature importances for better visualization\n",
    "\n",
    "plt.figure(figsize = (12,8))\n",
    "feat_importances = pd.Series(selection.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(20).plot(kind='barh')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d9d388",
   "metadata": {},
   "source": [
    "# Fitting model using Random Forest\n",
    "#1) Split dataset into train and test set in order to prediction w.r.t X_test\n",
    "\n",
    "#2) If needed do scaling of data\n",
    "\n",
    "#3) Scaling is not done in Random forest\n",
    "\n",
    "#4) Import model\n",
    "\n",
    "#5) Fit the data\n",
    "\n",
    "#6) Predict w.r.t X_test\n",
    "\n",
    "#7) In regression check RSME Score\n",
    "\n",
    "#8) Plot graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c3b0f18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "70f334e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "reg_rf = RandomForestRegressor()\n",
    "reg_rf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8413b4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reg_rf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "676224b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_rf.score(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5ec5d2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_rf.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6cd9cfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y_test-y_pred)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b99dcdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_pred, alpha = 0.5)\n",
    "plt.xlabel(\"y_test\")\n",
    "plt.ylabel(\"y_pred\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a53ca09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fb3ce353",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "87af61b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE/(max(DV)-min(DV))\n",
    "\n",
    "2090.5509/(max(y)-min(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7c1ebfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1193e776",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "Choose following method for hyperparameter tuning\n",
    "RandomizedSearchCV --> Fast\n",
    "GridSearchCV\n",
    "Assign hyperparameters in form of dictionery\n",
    "Fit the model\n",
    "Check best paramters and best score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "080a4b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b5e3faa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomized Search CV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10, 15, 100]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 5, 10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e40a60fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the random grid\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4aa4e200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search of parameters, using 5 fold cross validation, \n",
    "# search across 100 different combinations\n",
    "rf_random = RandomizedSearchCV(estimator = reg_rf, param_distributions = random_grid,scoring='neg_mean_squared_error',n_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cf0358",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9441004",
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_jobs=1,\n",
    "                   param_distributions={'max_depth': [5, 10, 15, 20, 25, 30],\n",
    "                                        'max_features': ['auto', 'sqrt'],\n",
    "                                        'min_samples_leaf': [1, 2, 5, 10],\n",
    "                                        'min_samples_split': [2, 5, 10, 15,\n",
    "                                                              100],\n",
    "                                        'n_estimators': [100, 200, 300, 400,\n",
    "                                                         500, 600, 700, 800,\n",
    "                                                         900, 1000, 1100,\n",
    "                                                         1200]},\n",
    "                   random_state=42, scoring='neg_mean_squared_error',\n",
    "                   verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971a72e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e1c2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = rf_random.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941218f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "sns.distplot(y_test-prediction)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c14fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "plt.scatter(y_test, prediction, alpha = 0.5)\n",
    "plt.xlabel(\"y_test\")\n",
    "plt.ylabel(\"y_pred\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35efedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE:', metrics.mean_absolute_error(y_test, prediction))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, prediction))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, prediction)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f2ab22",
   "metadata": {},
   "source": [
    "# Save the model to reuse it again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a4c3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# open a file, where you ant to store the data\n",
    "file = open('flight_rf.pkl', 'wb')\n",
    "\n",
    "# dump information to that file\n",
    "pickle.dump(reg_rf, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b142a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = open('flight_rf.pkl','rb')\n",
    "forest = pickle.load(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af10c013",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = forest.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff99908c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(y_test, y_prediction)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
